server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: false
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: false
    # python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' is the username and 'key' is the password for basic auth by default
    # If the auth is enabled, this value must be set in the "Authorization" header of the request.
    secret: "Basic c2VjcmV0OmtleQ=="

data:
  local_data_folder: local_data/private_gpt

ui:
  enabled: true
  path: /

llm:
  mode: local

vectorstore:
  #Can be chroma or qdrant
  database: chroma #qdrant #chroma

qdrant:
  path: local_data/private_gpt/qdrant

local:
  #Some models to test, coment/uncomment and run 'poetry run python scripts/setup' to download the models if not already downloaded
  #llm_hf_repo_id: TheBloke/Llama-2-13B-chat-GGUF
  #llm_hf_model_file: llama-2-13b-chat.Q5_K_M.gguf
  #llm_hf_repo_id: TheBloke/Llama-2-7B-Chat-GGUF
  #llm_hf_model_file: llama-2-7b-chat.Q5_K_M.gguf
  #llm_hf_repo_id: TheBloke/Llama-2-7B-32K-Instruct-GGUF  # NOT GREAT
  #llm_hf_model_file: llama-2-7b-32k-instruct.Q5_K_M.gguf
  llm_hf_repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
  llm_hf_model_file: mistral-7b-instruct-v0.1.Q4_K_M.gguf
  #llm_hf_repo_id: TheBloke/SciPhi-Self-RAG-Mistral-7B-32k-GGUF
  #llm_hf_model_file: sciphi-self-rag-mistral-7b-32k.Q4_K_M.gguf
  embedding_hf_model_name: BAAI/bge-small-en-v1.5

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  #Run 'export OPENAI_API_KEY="xxxxxx"' to set this as an environment variable, do not commit secrets to the repo
  api_key: ${OPENAI_API_KEY:}
  #Comment/uncomment to change OpenAI model
  #model: "gpt-3.5-turbo"
  #model: "gpt-3.5-turbo-16k"
  model: "gpt-4"
  #model: "gpt-4-32k"
